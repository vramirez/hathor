fail= sc.textFile("../tuit_aut.csv")
v1=("arroba","fecha","texto","nada")
p1=fail.map(lambda line: line.split(","))
df1=p1.toDF(v1)
training = sqlContext.sql("select distinct double(1) as label,lower(texto) as minus from tuits where lower(texto) like '%me encanta%' or lower(texto) like '%excelente%' or lower(texto) like '%maravill%' or lower(texto) like '% buen%' or lower(texto) like '%que bien%' or texto like '%:)%' or lower(texto) like '%:d' or lower(texto) like '%:d %' or texto like '%;)%' union select distinct double(0) as label,lower(texto) as minus from tuits where lower(texto) like '%hijue%' or lower(texto) like '%malparid%' or lower(texto) like '%pésim%' or lower(texto) like '%puta%' or lower(texto) like '%mierda%' or lower(texto) like '%pesim%'")
training.groupby("label").count().show()
test = sqlContext.sql("select *,lower(texto) minus from tuits where texto is not null")
#tokenizer = Tokenizer(inputCol="minus", outputCol="words")
tokenizer = RegexTokenizer(inputCol="minus", outputCol="words", pattern="\\W+",minTokenLength=3)
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
nb = NaiveBayes(smoothing=1.0, modelType="multinomial", labelCol="label")
pipeline = Pipeline(stages=[tokenizer, hashingTF, nb])
model = pipeline.fit(training)
prediction = model.transform(test)
#Según la documentación, salen ordenados, por eso hago filtro para consultar
#prediction.select("id", "text", "words").take(10)
prediction.groupby("prediction").count().show()
